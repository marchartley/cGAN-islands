\section{Learning-based generation}
\label{sec:coral-island_learning-based}


In this section, we introduce the use of a conditional Generative Adversarial Network (cGAN), specifically the pix2pix model proposed by \cite{Isola2017}, to enhance the island generation process by increasing the variety and flexibility of terrains. While the curve-based modeling algorithm can create numerous island examples, cGAN provides additional flexibility in generating more complex terrain without the rigid constraints of the curve-based algorithm that stem from our initial assumptions based on coral reef formation theory. 

% The pix2pix model was originally pretrained using RGB images. In this training phase, the images were labeled using the HSV (Hue, Saturation, Value) color space, where the Hue component specifically carried the label information. Both the Saturation and Value components were kept neutral, meaning they did not convey any significant label-related data. 
% 
% For the purpose of fine-tuning the model, we retained the use of the Hue component to encode the labels  directly from the parametric distance $H = \floor{\distRegions}$. We introduced a new dimension to the model's learning capabilities by incorporating the subsidence rate into the Value component $V =\subsidRate$. This addition not only utilizes the model's existing capability to interpret the HSV format but also enriches the input data, which now carries additional, valuable environmental information.

The training was performed using a batch size of 1, and optimized using the Adam optimizer with a learning rate of 0.0002 and momentum parameters $\beta_1 = 0.5$ and $\beta_2 = 0.999$. The loss function combined a conditional adversarial loss and an L1 loss, where the L1 loss was weighted by a factor $\lambda = 100$. The generator follows a U-Net architecture with encoder-decoder layers and skip connections, while the discriminator was based on a PatchGAN, classifying local image patches. These parameters are directly used from the original paper.

The inference of our model, with a dataset of approximatively 10 000 examples representing about 30 minutes of training, still outputs grid artifacts, visible in \cref{fig:coral-island_first-epoch}. After the second epoch, visual artifacts are already rare. This training time seems long but, at the best of our knowledge, is the shortest training time for terrain generation with learning-based approach in the litterature, mostly due to the mathematical nature of our dataset.

\begin{figure}
    \autofitgraphics[]{epoch-1-output.png, epoch-2-output.png, epoch-3-output.png} %, epoch-1-render.png, epoch-2-output.png, epoch-2-render.png}
    \caption{After the first epoch (left), grid artifacts similar to low resolution image are visible, but are being corrected during a second epoch (middle). After the second epoch, erosion patterns appear in the hieght fields (right).}
    \label{fig:coral-island_first-epoch}
\end{figure}


The Python script for the initial island dataset generation is poorly optimized and takes about 2.5s per island of size $256 \times 256$ as the parallelization does not take place here. Implementing an optimized C++ version of the initial generation process reduces this execution time to 50ms per generation.

On the other hand, the inference time of our deep learning model for a single input image of dimension $256 \times 256$ is constant whatever the complexity of the scene. Using the NVIDIA GeForce GTX 1650 Ti GPU with Python 3.10 and PyTorch version 2.5.1+cu121, the inference time measured is 5ms (std 1.1ms). 

% We not only show that using a neural network reduces the constraints on the generation process, but also that the execution time is only dependant on the network architecture, without influence from the dataset generation algorithm. 